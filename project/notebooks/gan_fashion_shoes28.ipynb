{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import PIL\n",
    "import time\n",
    "import datetime\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2DTranspose as C2DT\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#TODO:\n",
    "\n",
    "# Try dataset with colors (CIFAR10 almost done)\n",
    "# Try dataset with higher resolution\n",
    "# Create a dataset with only one category (shoes?)\n",
    "# Create more samples (Flip, rotate, skew..)\n",
    "\n",
    "\n",
    "# NOTES:\n",
    "# Might want to use Keras own image preprocessing functions\n",
    "# when taking in a whole directory of images in png format.\n",
    "# see https://keras.io/api/preprocessing/image/.\n",
    "\n",
    "# Grade system (loosely based on Goodwins feedback)\n",
    "# C - Color and medium resolution, with a filled in report (Master template with some text on each chapter)\n",
    "# B - High resolution, and proof of concept for further research (place new clothes on models)\n",
    "# A - Innovative\n",
    "\n",
    "\n",
    "# GPU workaround\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "### CONFIG ###\n",
    "EPOCHS = 50\n",
    "NOISE_DIM = 100\n",
    "NUM_EXAMPLES = 16\n",
    "\n",
    "BUFFER_SIZE = 40000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "IMAGE_PATH = \"imgs\"\n",
    "MODELS_PATH = \"models\"\n",
    "\n",
    "\n",
    "### INIT ###\n",
    "seed = tf.random.normal([NUM_EXAMPLES, NOISE_DIM])\n",
    "now = datetime.datetime.now().strftime(\"%d%H%M%S\")\n",
    "folder = os.path.join(IMAGE_PATH, now)\n",
    "MODELS_FOLDER = os.path.join(MODELS_PATH, now)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "# Make sure folders exists\n",
    "if not os.path.exists(MODELS_PATH):\n",
    "    os.mkdir(MODELS_PATH)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "    os.mkdir(MODELS_FOLDER)\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "if not os.path.exists(IMAGE_PATH):\n",
    "    os.mkdir(IMAGE_PATH)    \n",
    "\n",
    "def get_dataset():\n",
    "    shoes_path = \"data/fashion-shoes-28\"\n",
    "    shoes = os.listdir(shoes_path)\n",
    "    images = []\n",
    "    for k in shoes:\n",
    "        if k.startswith(\".\"):\n",
    "            continue\n",
    "        img_path = os.path.join(shoes_path, k)\n",
    "        \n",
    "        img = image.load_img(img_path, target_size=(28,28,1), color_mode=\"grayscale\")\n",
    "        img = image.img_to_array(img)\n",
    "#         img = img/-255 #invert grayscale\n",
    "        images.append(img)\n",
    "\n",
    "    images = np.asarray(images)\n",
    "    \n",
    "    print(\"Number of images:\", images.shape)\n",
    "    train_images = images.reshape(images.shape[0], 28, 28, 1).astype('float32')\n",
    "    train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "def plot_image(size):\n",
    "    \n",
    "    shoes_path = \"data/fashion-shoes-\"+str(size)\n",
    "    shoes = os.listdir(shoes_path)\n",
    "   \n",
    "    img_path = os.path.join(shoes_path, shoes[2])\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(size,size,1), color_mode=\"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/-255 #invert grayscale\n",
    "    \n",
    "    img = img.reshape(size,size)\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "    model.add(C2DT(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(C2DT(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(C2DT(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = (real_loss * 0.9) + fake_loss\n",
    "    print(\"total:\", total_loss)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    G_optimizer.apply_gradients(zip(gradients_of_generator, \n",
    "        generator.trainable_variables))\n",
    "    D_optimizer.apply_gradients(zip(gradients_of_discriminator, \n",
    "        discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n",
    "\n",
    "\n",
    "def train_forever(dataset):\n",
    "    epoch = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            start = time.time()\n",
    "\n",
    "            for image_batch in dataset:\n",
    "                train_step(image_batch)\n",
    "        \n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "                display.clear_output(wait=True)\n",
    "                generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "            print ('Time for epoch {} is {:.4f} sec'.format(epoch + 1, time.time()-start))\n",
    "            epoch += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KeyboardInterrupt. Stopping training.\")\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 2, seed)\n",
    "\n",
    "\n",
    "def generate_and_save_images(model, epoch, seed):\n",
    "    predictions = model(seed, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('{}/image_at_epoch_{:05d}.png'.format(folder, epoch))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save generator model\n",
    "    filename = '{}/generator_model_{:05d}.h5'.format(MODELS_FOLDER, epoch)\n",
    "    model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAQtklEQVR4nO3dX4hcZZrH8d9j0iaaaP7YbWgctWclF4q4maEICxpxGVbUG52LWRUZoggtouiA+Ifxz+idLjqDF+tAZtWJq+s4MhMUDeuIjIS5GdIGzR9j1myMY0wn6SDGRCUm8dmLLpee2Od5yzp16pzk/X6gqe56+tR5u6p/XV31nPe85u4CcPw7oe4BAOgPwg5kgrADmSDsQCYIO5CJmf3c2eDgoI+MjPRzl0BWtm/frr1799p0tVJhN7PLJD0uaYak/3D3h6PvHxkZ0djYWJldVqZMC9Js2vu249suu32dUmNHf7VarcJa1//Gm9kMSf8u6XJJ50m61szO6/b2AFSrzGv2pZK2uvs2d/9K0u8kXdmbYQHotTJhP0PSR1O+3tG+7u+Y2aiZjZnZ2MTERIndASijTNine7H2rReX7r7C3Vvu3hoaGiqxOwBllAn7DklnTvn6e5J2lhsOgKqUCftaSYvN7PtmdqKkayS93JthAei1rltv7n7YzG6V9JomW29PufumDrYrrNXZgiqz72O5NVZ1W5DWXHOU6rO7+2pJq3s0FgAV4nBZIBOEHcgEYQcyQdiBTBB2IBOEHchEX+ezp5Tp2ZbtdVfZL07ddp296mN5ei2+G57ZgUwQdiAThB3IBGEHMkHYgUwQdiATjWq9pdTZBqrz7LO0v9ALPLMDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5CJY6rPXqUqp3o2uU/OFNd88MwOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAm6LM3QNlTRZdZBhv5KBV2M9suab+kI5IOu3urF4MC0Hu9eGb/Z3ff24PbAVAhXrMDmSgbdpf0JzN7y8xGp/sGMxs1szEzG5uYmCi5OwDdKhv2C939h5Iul3SLmV189De4+wp3b7l7a2hoqOTuAHSrVNjdfWf7co+kVZKW9mJQAHqv67Cb2RwzO+WbzyVdKmljrwYGoLfKvBu/SNKqdh93pqT/cvf/7smoupDqJ3/99delbj/a/oQTyr0aqnLOOPPR8Y2uw+7u2yT9Yw/HAqBCtN6ATBB2IBOEHcgEYQcyQdiBTPR9imuZKZdRGynVYvryyy/Deqp9Fm0/MDAQbjt37tyw3mRMkT1+8MwOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAm+t5nr2rK5cGDB8P6Rx99VOr29+zZU1hL9dEXL14c1mfPnh3WU8cIHDhwoLA2Z86ccNuvvvoqrM+fPz+sz5wZ/wpFffqyvwtVLjdd9rabeHwCz+xAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSiUUs2lzkddKrPnuqLfvLJJ2F927ZthbVUL/vNN98M6zt37gzry5YtC+vvvvtuYe3w4cPhtqn75bbbbgvr8+bNC+uRKvvkqduvusffRDyzA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQiUbNZ0/1LqN6attNmzaF9fPPPz+sb9mypbD2yiuvhNu++OKLYT01Xz3Vh3/ggQcKa7t27Qq3nTFjRlg/dOhQWE8dnxDNh0/tu8pedtklvssu012H5IjN7Ckz22NmG6dct9DMXjez99uXC6odJoCyOvnz9FtJlx113T2S3nD3xZLeaH8NoMGSYXf3NZKO/l/tSkkr25+vlHRVj8cFoMe6feGxyN3HJal9eXrRN5rZqJmNmdnYxMREl7sDUFbl7zK4+wp3b7l7a2hoqOrdASjQbdh3m9mwJLUvi0+9CqARug37y5KWtz9fLuml3gwHQFWSfXYze17SJZIGzWyHpF9IeljS783sRkl/k/STTncY9TdTvc1o21Tf88iRI2H9kUceCeuvvfZaYS06p7wkzZo1K6ynzhufeq/j3nvvLazddddd4bap4wvGx8fD+kknnRTWo/MMpH7usr3s6DFP9fiPR8mwu/u1BaUf9XgsACp07B0GBKArhB3IBGEHMkHYgUwQdiATfZ/iWtXpfaPWmCRt3rw5rK9atSqsRy2kc845J9z2jjvuCOupZZNXrlwZ1j/44IPC2uOPPx5ue9ZZZ4X1hx56KKzv2LEjrH/44YeFtUWLFoXbLlgQT6ZMTVON2mup37VU248lmwE0FmEHMkHYgUwQdiAThB3IBGEHMkHYgUwcU6eSjrzzzjth/bHHHgvrqVMmR/3o5557Ltw2Nb12w4YNYT3Vp1+zZk1hbcWKFaVu+7777gvrd955Z1j/4osvCmv79u0Lt009Jqk+fZnjNrI8lTSA4wNhBzJB2IFMEHYgE4QdyARhBzJB2IFM9L3PXpXbb789rO/fvz+sv/DCC2E9OtX0nDlzwm1POeWUsJ5a9jg1Z/ymm24qrC1dujTcdnR0NKzff//9Yf3ZZ58N69dff31hLXWK7a1bt4b1devWhfWLL764sHbyySeH2x6LffSU4+8nAjAtwg5kgrADmSDsQCYIO5AJwg5kgrADmbAyc36/q1ar5WvXrq3ktnfv3h3WU+dmP3z4cNf7PnDgQFhPLQ88d+7csJ7qRw8MDBTWUvOyb7jhhrD+6quvltp+5sziQznuvvvucNv58+eH9V27doX16PiH1FLTqcck1aev67zxrVZLY2Nj0+48+cxuZk+Z2R4z2zjlugfN7GMze7v9cUUvBwyg9zr5N/63ki6b5vpfufuS9sfq3g4LQK8lw+7uayTFx3MCaLwyb9Ddambr2//mFy7KZWajZjZmZmMTExMldgegjG7D/mtJ50haImlcUuHZHN19hbu33L01NDTU5e4AlNVV2N19t7sfcfevJf1GUjy1CkDtugq7mQ1P+fLHkjYWfS+AZkjOZzez5yVdImnQzHZI+oWkS8xsiSSXtF1S8YTqPhkcHAzrn376aVjfu3dvWI965cPDw4U1KT2XPnWMwLx588J61PN95plnwm1Xr44bKSeeeGJYT60df+qppxbWli9fHm6betl39tlnh/XoGIPU4/3000+H9SVLloT1Sy+9NKxH8+WrOvYlGXZ3v3aaq5+sYCwAKsThskAmCDuQCcIOZIKwA5kg7EAmjqlTSUfTBqOplJJ02mmnhfXU6Z4/++yzwlqqrTd79uywPjIyEtZTp5p+4oknCmuPPvpouG2qzZNabjq1/eeff15Yu/nmm8NtFywoPApbUnqa6ccff9xVTYofbylewltKLyEeTbFNnca62+mzPLMDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5CJRvXZU/3DqKeb6vembjs1lXPhwoWFtdSSzak+fKqeOgbguuuuK6xFU0wlaf369WF9y5YtYT3Vr45Os/3ee++F26Yek9RjHp0OOnW/LFu2LKxHS1F3IuqlV3Uaap7ZgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IRKP67GVOoVv1ErlRXzS1/G+qX5zqo6eWzYqWm77mmmvCba+++uqwnnpMUqfJjpbKTt32oUOHwnrqMY/m4qf67KljJ1KPWWqZ7jJSy3AX4ZkdyARhBzJB2IFMEHYgE4QdyARhBzJB2IFMNKrPXqZXXnaZ2yrn0qd6rtG8ayl93vlo+eHUctAps2bNCuupsUfHAKTul4MHD4b1Mn321M+Vkup1p879XkZl5403szPN7M9mttnMNpnZ7e3rF5rZ62b2fvsyPqM/gFp18ufnsKQ73P1cSf8k6RYzO0/SPZLecPfFkt5ofw2goZJhd/dxd1/X/ny/pM2SzpB0paSV7W9bKemqqgYJoLzv9MLCzEYk/UDSXyUtcvdxafIPgqTTC7YZNbMxMxtLHeMNoDodh93M5kr6g6SfuXu86t0U7r7C3Vvu3hoaGupmjAB6oKOwm9mAJoP+nLv/sX31bjMbbteHJe2pZogAeiHZerPJ9/mflLTZ3X85pfSypOWSHm5fvtTJDsu2yLpVdgps1VNoI6nlqIeHhwtrqaWq9+3bF9ZTp3tOnUo6agtGyzlL6anDqfZXVE9NUT333HPD+gUXXBDWU9Oay/w+dbttJ332CyX9VNIGM3u7fd3PNRny35vZjZL+JuknXY0AQF8kw+7uf5FU9KfkR70dDoCqcLgskAnCDmSCsAOZIOxAJgg7kIm+T3GNpv7V1YPvZN919tlTon7ywMBAuO3g4GBYv+iii7oa0zeiaabR9FcpPcU1um2p3LLIqR5/6tiHOpcfL8IzO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmeh7n72uZZmP5T56amxRvc5TbEtxPzp1KunUKbRTorGV/bmq/H2p6jTUPLMDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5CJRi3ZXKU6++hle7pNnudfRtWPSR3nZm/K7U+HZ3YgE4QdyARhBzJB2IFMEHYgE4QdyARhBzKRDLuZnWlmfzazzWa2ycxub1//oJl9bGZvtz+uKDsYMws/jlXuHn6kfu4qP1KO18ckR50cVHNY0h3uvs7MTpH0lpm93q79yt0frW54AHqlk/XZxyWNtz/fb2abJZ1R9cAA9NZ3es1uZiOSfiDpr+2rbjWz9Wb2lJktKNhm1MzGzGxsYmKi1GABdK/jsJvZXEl/kPQzd/9M0q8lnSNpiSaf+R+bbjt3X+HuLXdvDQ0N9WDIALrRUdjNbECTQX/O3f8oSe6+292PuPvXkn4jaWl1wwRQVifvxpukJyVtdvdfTrl+eMq3/VjSxt4PD0CvdPJu/IWSfippg5m93b7u55KuNbMlklzSdkk3VTLCKeo6DXXVqj7dc5lt65xei97q5N34v0ia7jdide+HA6AqHEEHZIKwA5kg7EAmCDuQCcIOZIKwA5nI5lTSKU1e0rnJxwjg2MEzO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmbB+zlc2swlJH065alDS3r4N4Ltp6tiaOi6JsXWrl2M7292nPf9bX8P+rZ2bjbl7q7YBBJo6tqaOS2Js3erX2Pg3HsgEYQcyUXfYV9S8/0hTx9bUcUmMrVt9GVutr9kB9E/dz+wA+oSwA5moJexmdpmZbTGzrWZ2Tx1jKGJm281sQ3sZ6rGax/KUme0xs41TrltoZq+b2fvty2nX2KtpbD1fxrvLsRUtM17rfdfP5c+n3X+/X7Ob2QxJ/yPpXyTtkLRW0rXu/m5fB1LAzLZLarl77QdgmNnFkg5Iesbdz29f92+SPnH3h9t/KBe4+90NGduDkg7UvYx3e7Wi4anLjEu6StL1qvG+C8b1r+rD/VbHM/tSSVvdfZu7fyXpd5KurGEcjefuayR9ctTVV0pa2f58pSZ/WfquYGyN4O7j7r6u/fl+Sd8sM17rfReMqy/qCPsZkj6a8vUONWu9d5f0JzN7y8xG6x7MNBa5+7g0+csj6fSax3O05DLe/XTUMuONue+6Wf68rDrCPt0J1ZrU/7vQ3X8o6XJJt7T/XUVnOlrGu1+mWWa8Ebpd/rysOsK+Q9KZU77+nqSdNYxjWu6+s325R9IqNW8p6t3frKDbvtxT83j+X5OW8Z5umXE14L6rc/nzOsK+VtJiM/u+mZ0o6RpJL9cwjm8xszntN05kZnMkXarmLUX9sqTl7c+XS3qpxrH8naYs4120zLhqvu9qX/7c3fv+IekKTb4j/7+S7q1jDAXj+gdJ77Q/NtU9NknPa/LfukOa/I/oRkmnSXpD0vvty4UNGtt/Stogab0mgzVc09gu0uRLw/WS3m5/XFH3fReMqy/3G4fLApngCDogE4QdyARhBzJB2IFMEHYgE4QdyARhBzLxf4nhvrARKSRzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 2070 is 0.9805 sec\n",
      "Time for epoch 2071 is 0.7800 sec\n",
      "Time for epoch 2072 is 0.7811 sec\n",
      "Time for epoch 2073 is 0.7831 sec\n",
      "KeyboardInterrupt. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_dataset   = get_dataset()\n",
    "    generator       = make_generator_model()\n",
    "    discriminator   = make_discriminator_model()\n",
    "\n",
    "    cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    # learningrate started at 1e-4\n",
    "    G_optimizer = Adam(0.00001)\n",
    "    D_optimizer = Adam(0.00004)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(generator_otimizer=G_optimizer,\n",
    "                                    discriminator_optimizer=D_optimizer,\n",
    "                                    generator=generator, discriminator=discriminator)\n",
    "\n",
    "    #train(train_dataset, EPOCHS)\n",
    "    train_forever(train_dataset)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
